{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67ae8d2077eaa51f",
   "metadata": {},
   "source": [
    "# Server connection check"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:27.435391Z",
     "start_time": "2025-01-27T11:08:26.619737Z"
    }
   },
   "source": [
    "from typing import Any\n",
    "\n",
    "from twisted.mail.smtp import SUCCESS\n",
    "\n",
    "# In this notebook we have trained the pytorch models for the adversarial and XAI data of the FGSM attack conducted on the DeepMIMO dataset. This notebook only contains the Pytorch based neural network.\n",
    "\n",
    "print('hello')\n",
    "import os\n",
    "from sys import executable as exec\n",
    "print('current working directory: ', os.getcwd())\n",
    "print('current conda environment: ', exec)\n",
    "\n",
    "#--notebook-dir=/home/spatial01/RND/adversarial-detector/adversarial-detector-xai"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "current working directory:  /home/spatial01/RND/adversarial-detector/adversarial-detector-xai\n",
      "current conda environment:  /home/spatial01/anaconda3/envs/netslab_lime_defense/bin/python\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "2953472cfeb1e312",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:27.485041Z",
     "start_time": "2025-01-27T11:08:27.473767Z"
    }
   },
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "## Testing if Torch is working correclty\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2909, 0.4198, 0.4386],\n",
      "        [0.6161, 0.0260, 0.7284],\n",
      "        [0.9474, 0.8516, 0.4202],\n",
      "        [0.6345, 0.9185, 0.9822],\n",
      "        [0.2324, 0.7525, 0.2175]])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "4620da4d8f36abe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:28.204190Z",
     "start_time": "2025-01-27T11:08:28.152486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Eager execution shit in tensorflow\n",
    "\n",
    "import os\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import deprecation\n",
    "from pprint import pp as pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %% Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# reload and load custom modules\n",
    "import importlib\n",
    "import myutils\n",
    "import codb_utils\n",
    "importlib.reload(codb_utils)\n",
    "importlib.reload(myutils)\n",
    "from myutils import create_dir_if_not_exist, get_latest_file, to_dataLoader, PyTorchPredWrapper, load_latest_model, get_latest_model, conv_double_to_single_col_binary_labels\n",
    "\n",
    "# Experiment loop\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# pytorch\n",
    "from torch import optim, nn, Tensor\n",
    "import torch\n",
    "import lightning as L\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [12, 8] # Set default plot size\n",
    "\n",
    "# paths\n",
    "# Change the date here to use the files from a different date\n",
    "proj_root = os.getcwd()\n",
    "# Loading datasets\n",
    "load_dlcb_data_dir = os.path.join(os.getcwd(), \"data/DeepMIMO_data/DLCB_dataset_21-07-24/\")\n",
    "load_adv_fgsm_data_dir = os.path.join(proj_root, \"results/Novel_DMIMO_21-03-24/AdvData/\")\n",
    "load_adv_cw_data_dir = os.path.join(proj_root, \"results/Novel_DMIMO_28-04-24/AdvData/\")\n",
    "load_all_adv_data_dir = os.path.join(proj_root, \"results/Novel_DMIMO_02-05-24/AdvData/\")\n",
    "load_xai_data_dir = os.path.join(proj_root, \"results/Novel_DMIMO_12-04-24/Explanations/\")\n",
    "# Loading logs\n",
    "load_lighting_logs_dir = os.path.join(proj_root, \"lightning_logs/\")\n",
    "# Loading results and observations\n",
    "load_results_dir = os.path.join(proj_root, \"results/Novel_DMIMO_15-04-24/Observations/\")\n",
    "# Loading trained models\n",
    "load_adv_detector_dir = os.path.join(proj_root, \"results/Novel_DMIMO_11-04-24/Models/adv_detector/\")\n",
    "load_dlcb_models_dir = os.path.join(proj_root, \"results/Novel_DMIMO_27-01-25 - EBY/Models/dlcb_models/\")\n",
    "# EBY data directories\n",
    "load_eby_data_dir_v1 = os.path.join(os.getcwd(), \"data/EBY_data/V1/\")\n",
    "\n",
    "path_base=os.path.join(os.getcwd(), \"results/Novel_DMIMO_\")\n",
    "# Create directories\n",
    "results_dir = create_dir_if_not_exist(os.path.join(os.getcwd(), \"results/Novel_DMIMO_\"+dt.datetime.now().strftime(\"%d-%m-%y\") + \" - EBY\"))\n",
    "results_figs_dir = create_dir_if_not_exist(os.path.join(results_dir, 'Figs'))\n",
    "results_data_dir = create_dir_if_not_exist(os.path.join(results_dir, 'Observations'))\n",
    "results_adv_data_dir = create_dir_if_not_exist(os.path.join(results_dir, 'AdvData'))\n",
    "results_explanations_dir = create_dir_if_not_exist(os.path.join(results_dir, 'Explanations'))\n",
    "results_models_dir = create_dir_if_not_exist(os.path.join(results_dir, 'Models'))\n",
    "results_xai_detector_dir = create_dir_if_not_exist(os.path.join(results_models_dir, 'xai_detector'))\n",
    "results_adv_detector_dir = create_dir_if_not_exist(os.path.join(results_models_dir, 'adv_detector'))\n",
    "results_dlcb_models_dir = create_dir_if_not_exist(os.path.join(results_models_dir, 'dlcb_models'))\n",
    "\n",
    "# CONFIGURATION\n",
    "# Make it False to train the model\n",
    "LOAD_XAI_ADVERSARIAL_MODEL_FROM_FILE = False\n",
    "modifier_txt_direct_adv_det = 'Full_direct_adv_DF, lr 1e-3, three_layers'\n",
    "modifier_txt_xai_adv_det = 'Full_XAI_DF, lr 1e-3'\n",
    "modifier_txt_xgboost_zero_day='Unseen_XGBoost_Tr-FGSM_Ts-FGSM-CWL2'\n",
    "modifier_txt_skl_mlp_zero_day='Unseen_SKL_MLP_Tr-FGSM_Ts-FGSM-CWL2'\n",
    "modifier_txt_skl_mlp_adv_det = 'SKL_MLP_Tr-FGSM_Ts-FGSM'\n",
    "modifier_txt_xgboost_adv_det = 'XGBoost_Tr-FGSM_Ts-FGSM'\n",
    "# Make it False to train the model\n",
    "LOAD_DIRECT_ADVERSARIAL_MODEL_FROM_FILE = False\n",
    "TRAIN_XGBOOST_ADV_ZERO_DAY=True\n",
    "TRAIN_SKL_MLP_ADV_ZERO_DAY=True\n",
    "# Make this 2 or 3 for testing and 1.0 (float) for the full dataset\n",
    "CONFIG_TRAIN_LIMIT = 1.0\n",
    "# Batch size for both direct and XAI detectors\n",
    "DMAD_BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "data_directories = {\n",
    "    'dlcb_data_load': load_dlcb_data_dir,\n",
    "    'dlcb_model_save': results_dlcb_models_dir,\n",
    "    'dlcb_model_load': load_dlcb_models_dir,\n",
    "    # 'dlcb_model_load': results_dlcb_models_dir, # this line loads the models from the currently saving folder. To use a fixed folder, use the above commented line\n",
    "    'fgsm_data_load': load_adv_fgsm_data_dir,\n",
    "    'xai_data_load': load_xai_data_dir,\n",
    "    'adv_data_save': results_adv_data_dir,\n",
    "    'xai_data_save': results_explanations_dir,\n",
    "    'eby_data_load': load_eby_data_dir_v1\n",
    "}\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    importlib.reload(tf)\n",
    "except ValueError as ve:\n",
    "    print(\"Eager execution is already enabled: \", ve)\n",
    "\n",
    "MODEL_TRAINING = False\n",
    "if MODEL_TRAINING:\n",
    "    LOAD_MODEL=False\n",
    "    # tf.compat.v1.disable_eager_execution()\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "elif not MODEL_TRAINING:\n",
    "    LOAD_MODEL=True\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    CREATE_ATTACK_DATASETS = LOAD_MODEL\n",
    "# Change this when you want to train the DLCB original tensorflow model\n",
    "try:\n",
    "    # Disable all GPUS\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n",
    "\n",
    "\n",
    "# Stop running the main loop\n",
    "RUN_LOOP = False\n",
    "GENERATE_EXPL_PLOTS = False\n",
    "TDD_loop = False\n",
    "RUNTIME_VIZ = False\n",
    "\n",
    "######### Warning\n",
    "## It is pertinent that you use the following versions of the packages\n",
    "# tensorflow==2.14.0\n",
    "# keras==2.14.0\n",
    "# tensorflow-addons>=0.13.0\n",
    "# art==1.17.0\n",
    "# Otherwise it might give trouble with placeholder and tensorflow v2 related issues.\n",
    "# The IDS model used must have the output shape with Dense(2)\n",
    "# art generator takes numpy arrays only for generation. No pandas dataframes to generate_adversarial_data method."
   ],
   "id": "efa2e1f2d896251d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution is already enabled:  tf.enable_eager_execution must be called at program startup.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adversarial data creation",
   "id": "a47ddf483972c72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:28.262479Z",
     "start_time": "2025-01-27T11:08:28.257432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from art.estimators.classification import KerasClassifier\n",
    "from codb_utils import train_codb_model, codb_train_test_split\n",
    "from myutils import evasion_attacks, NotFittedError, generate_art_based_evasion_data\n",
    "from pprint import pp as pprint\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod, ZooAttack, CarliniL2Method\n",
    "from art.attacks import EvasionAttack"
   ],
   "id": "8d00aa487f40f85e",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:28.382842Z",
     "start_time": "2025-01-27T11:08:28.351498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the data\n",
    "def load_all_data(folder_path):\n",
    "    # Dictionary to store DataFrames\n",
    "    dataframes_dict = {}\n",
    "    dos_label_map = {}\n",
    "\n",
    "    # Iterate through files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Store the DataFrame in the dictionary with the filename as the key\n",
    "            dict_key = file_name.split(\".\")[0]\n",
    "            dict_key = dict_key.split(\"_\")[-1]\n",
    "            if dict_key.endswith(\"N\"):\n",
    "                dataframes_dict[dict_key] = df, pd.DataFrame(np.zeros((df.shape[0], 1), dtype=int))\n",
    "                dos_label_map[0] = \"normal\"\n",
    "            elif dict_key.endswith(\"tcp\"):\n",
    "                dataframes_dict[dict_key] = df, pd.DataFrame(np.ones((df.shape[0], 1), dtype=int))\n",
    "                dos_label_map[1] = \"tcp\"\n",
    "            elif dict_key.endswith(\"dns\"):\n",
    "                dataframes_dict[dict_key] = df, pd.DataFrame(np.ones((df.shape[0], 1), dtype=int)*2)\n",
    "                dos_label_map[2] = \"dns\"\n",
    "            elif dict_key.endswith(\"ntp\"):\n",
    "                dataframes_dict[dict_key] = df, pd.DataFrame(np.ones((df.shape[0], 1), dtype=int)*3)\n",
    "                dos_label_map[3] = \"ntp\"\n",
    "\n",
    "    return dataframes_dict, dos_label_map\n",
    "\n",
    "# Prep the data\n",
    "def get_X_y_from_data_dict(data_dict):\n",
    "    dfs = []\n",
    "    ys = []\n",
    "    for _, (df, y) in data_dict.items():\n",
    "        dfs.append(df)\n",
    "        ys.append(y)\n",
    "    X = pd.concat(dfs, ignore_index=True)\n",
    "    y = pd.concat(ys, ignore_index=True)\n",
    "    return X, y\n",
    "\n",
    "def binarize_labels_for_ids(y):\n",
    "    # convert multi label y to binary labels\n",
    "    y_bin = np.zeros((y.shape[0], 1), dtype=int)\n",
    "    y_bin[y > 0] = 1\n",
    "    y_bin_2 = np.zeros((y_bin.shape[0], 2), dtype=int)\n",
    "    # expand y_bin to two columns with one hot encoding\n",
    "    for i, yb in enumerate(y_bin):\n",
    "        y_bin_2[i, yb] = 1\n",
    "    y_index = y\n",
    "    return y_bin_2, y_index, {0: \"normal\", 1: \"attack\"}\n",
    "\n",
    "def save_per_attk_adv_data(folder_path, modifier, X, y=None):\n",
    "    np.save(os.path.join(folder_path, f\"adv_X_{modifier}_{dt.datetime.now().strftime('%H-%M-%S')}.npy\"), X, allow_pickle=False)\n",
    "    if y is not None:\n",
    "        np.save(os.path.join(folder_path, f\"adv_y_{modifier}_{dt.datetime.now().strftime('%H-%M-%S')}.npy\"), y, allow_pickle=False)\n",
    "    return True"
   ],
   "id": "1b282c58f827386a",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:28.590142Z",
     "start_time": "2025-01-27T11:08:28.568798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def train_encoder_decoder_for_adv_gen(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2025)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=2025)\n",
    "\n",
    "    # Scale the data\n",
    "    # scaler = StandardScaler()\n",
    "    # X_train = scaler.fit_transform(X_train)\n",
    "    # X_test = scaler.transform(X_test)\n",
    "    input_shape = (X_train.shape[1],)\n",
    "    # Define the encoder-decoder model\n",
    "    # Encoder\n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "\n",
    "    # Decoder / Classifier\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss=BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        verbose=1,\n",
    "                        callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')])\n",
    "\n",
    "    # save the model\n",
    "    model_name = \"Ericsson-IDS\"\n",
    "    file_path=os.path.join(data_directories['dlcb_model_save'], f\"{model_name}_{dt.datetime.now().strftime('%H-%M-%S')}.keras\")\n",
    "    model.save(file_path)\n",
    "\n",
    "    split_data = [X_train, X_test, y_train, y_test]\n",
    "    # Evaluate the model\n",
    "    return model, split_data"
   ],
   "id": "427d39d3a904ad74",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:28.719317Z",
     "start_time": "2025-01-27T11:08:28.698317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_adversarial_data(ids_model, attack_param_dict, input_samples):\n",
    "    \"\"\"\n",
    "    Generates adversarial data for the given attack types and parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attack_param_dict: A dictionary containing the attack types and their parameters. Here the FastGradientMethod is of type art.attack.EvasionAttack type\n",
    "        attack_param_dict={\n",
    "                            'FastGradientMethod_0.2': (FastGradientMethod, {'eps':0.2}),\n",
    "                            'FastGradientMethod_0.3': (FastGradientMethod, {'eps':0.3})\n",
    "                        }\n",
    "    dataset: Default would be the test dataset in the dlcb_sub_datasets\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_adv_data_dict: A dictionary containing the adversarial data for each model and each label in the attack params dict\n",
    "    \"\"\"\n",
    "    print(f\"Generating adversarial data for model for IDS model\")\n",
    "    adv_wrapped_model = KerasClassifier(model=ids_model, use_logits=False)\n",
    "    adv_data_dict = {}\n",
    "    for attack_label, (attack_method, params) in tqdm(attack_param_dict.items()):\n",
    "        try:\n",
    "            print(f\"Generating {attack_label} attack dataset\")\n",
    "            adv_data_dict[attack_label] = generate_art_based_evasion_data(wrapped_model=adv_wrapped_model, attack_method=attack_method, X=input_samples, **params)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate the attack dataset for {attack_label} due to {e}\")\n",
    "            continue\n",
    "        # save right after the generation\n",
    "        save_data_directory = data_directories['adv_data_save']\n",
    "        model_name = f\"Ericsson-IDS\"\n",
    "        save_per_attk_adv_data(save_data_directory, f\"{model_name}_{attack_label}\", adv_data_dict[attack_label])\n",
    "        print(f\"[SUCCESS] Adversarial data for {attack_label} attack saved for Ericsson-IDS model\")\n",
    "    return adv_data_dict\n",
    "\n",
    "# write to save all the data adv and original and the model in one .npy file for the run session.\n",
    "def save_all_together(adv_data_dict, X, y_index, save_data_directory=None, **kwargs):\n",
    "    model_name = kwargs['model_name']; kwargs.pop('model_name', None) # remove key\n",
    "    if save_data_directory is None:\n",
    "        save_data_directory = data_directories['adv_data_save']\n",
    "    save_dict = {}\n",
    "    save_dict['adv_data_dict'] = adv_data_dict\n",
    "    save_dict['X'] = X\n",
    "    save_dict['y_index'] = y_index\n",
    "    np.save(os.path.join(save_data_directory, f\"AdvPack_{model_name}_{kwargs}_{dt.datetime.now().strftime('%H-%M-%S')}.npy\"), X, allow_pickle=False)\n",
    "    print(\"[SUCCESS] All the data is packed and saved successfully!\")\n",
    "    return True"
   ],
   "id": "bf5a0bef7ea04c92",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:30.029595Z",
     "start_time": "2025-01-27T11:08:28.861582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Specify the folder path\n",
    "folder_path = load_eby_data_dir_v1\n",
    "\n",
    "# Read CSV files and store them in a dictionary\n",
    "eby_all_data = load_all_data(load_eby_data_dir_v1)\n",
    "\n",
    "# Visualize the read data\n",
    "print(\"All data sizes: \", eby_all_data[0][\"N\"][1].shape)\n",
    "print(\"All data label_map: \", eby_all_data[1])\n",
    "\n",
    "# binarise the data for IDS\n",
    "X, y = get_X_y_from_data_dict(eby_all_data[0])\n",
    "y, y_index, bin_label_map = binarize_labels_for_ids(y)\n",
    "print(X.shape, y.shape)\n",
    "print(y.sum(axis=0))\n",
    "# Train the ids model\n",
    "if MODEL_TRAINING:\n",
    "    ids_model, split_data = train_encoder_decoder_for_adv_gen(X, y)\n",
    "    # Evaluate the IDS mode;\n",
    "    test_loss, test_accuracy = ids_model.evaluate(split_data[1], split_data[3], verbose=0)\n",
    "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "elif LOAD_MODEL:\n",
    "    load_path_extension=\"Ericsson-IDS*.keras\"\n",
    "    print(f\"Loading model {data_directories['dlcb_model_load']}\")\n",
    "    try:\n",
    "        latest_filename= get_latest_file(data_directories['dlcb_model_load'], load_path_extension)\n",
    "        ids_model = keras.models.load_model(latest_filename)\n",
    "        print(f\"Model Successfully Loaded from {latest_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model loading Failed from {latest_filename}\")\n",
    "        print(e)"
   ],
   "id": "5f6248b4c1f80420",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data sizes:  (8999, 1)\n",
      "All data label_map:  {2: 'dns', 1: 'tcp', 0: 'normal', 3: 'ntp'}\n",
      "(18965, 127) (18965, 2)\n",
      "[8999 9966]\n",
      "Loading model /home/spatial01/RND/adversarial-detector/adversarial-detector-xai/results/Novel_DMIMO_27-01-25 - EBY/Models/dlcb_models/\n",
      "WARNING:tensorflow:`compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n",
      "Model Successfully Loaded from /home/spatial01/RND/adversarial-detector/adversarial-detector-xai/results/Novel_DMIMO_27-01-25 - EBY/Models/dlcb_models/Ericsson-IDS_09-14-57.keras\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:08:30.283303Z",
     "start_time": "2025-01-27T11:08:30.273726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import art\n",
    "art.__version__"
   ],
   "id": "a768e053aa0869eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.0'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T11:11:02.222817Z",
     "start_time": "2025-01-27T11:10:55.068341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attack data generation\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod, ZooAttack, CarliniL2Method, DeepFool, BasicIterativeMethod, ProjectedGradientDescent, SaliencyMapMethod, TargetedUniversalPerturbation, UniversalPerturbation, Wasserstein, CarliniLInfMethod\n",
    "\n",
    "if CREATE_ATTACK_DATASETS:\n",
    "    # attacks_and_params={\n",
    "    #     'FastGradientMethod_0.2': (FastGradientMethod, {'eps':0.2})\n",
    "    #     , '': (FastGradientMethod, {'eps':0.2})\n",
    "    #     #, 'CarliniL2Method_2': (CarliniL2Method, {'max_iter':2, 'learning_rate':0.1})\n",
    "    # }\n",
    "    eps_val = 0.9\n",
    "    adv_datasize=\"Full\"\n",
    "    attacks_and_params={\n",
    "        'DeepFool_max_iter_10': (DeepFool, {'max_iter':10, 'batch_size':32, 'nb_grads':1, 'gen_kwargs': {'y': y}})\n",
    "        ,'CarliniLInfMethod': (CarliniLInfMethod, {'max_iter':2, 'learning_rate':0.1})\n",
    "        ,'UniversalPerturbation': (UniversalPerturbation, {'attacker':'fgsm', 'gen_kwargs': {'y':y}})\n",
    "        ,'TargetedUniversalPerturbation': (TargetedUniversalPerturbation, {'gen_kwargs': {'y':y}})\n",
    "        # ,'SaliencyMapMethod': (SaliencyMapMethod, {})\n",
    "        ,'ProjectedGradientDescent': (ProjectedGradientDescent, {'eps':eps_val})\n",
    "        ,'BasicIterativeMethod_0.2': (BasicIterativeMethod, {'eps':eps_val})    # The Basic Iterative Method is the iterative version of FGM and FGSM.\n",
    "        ,'FastGradientMethod_0.2': (FastGradientMethod, {'eps':eps_val})\n",
    "        ,'CarliniL2Method_0.2': (CarliniL2Method, {'confidence':eps_val})\n",
    "    }\n",
    "    adv_data_dict = generate_adversarial_data(ids_model, attacks_and_params, X.values)\n",
    "    save_all_together(adv_data_dict, X, y_index, save_data_directory=None, eps=eps_val, model_name=\"Ericsson-IDS\", data_size=adv_datasize)\n",
    "\n",
    "    exit()\n",
    "# for the full dataset use concatenate_sub_datasets('X') as the input for generate_adversarial_data function"
   ],
   "id": "cbdad1cb23aea092",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating adversarial data for model for IDS model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating DeepFool_max_iter_10 attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepFool:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d80528ef35da46f3bbd32b5ce27c3172"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for DeepFool_max_iter_10 attack saved for Ericsson-IDS model\n",
      "Generating CarliniLInfMethod attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C&W L_inf:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6451371833da4809969b92947b3ab16f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:03<00:11,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for CarliniLInfMethod attack saved for Ericsson-IDS model\n",
      "Generating UniversalPerturbation attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Universal perturbation:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1064e39e43ea46febcfd4bf85a94ad86"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for UniversalPerturbation attack saved for Ericsson-IDS model\n",
      "Generating TargetedUniversalPerturbation attack dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:03<00:03,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for TargetedUniversalPerturbation attack saved for Ericsson-IDS model\n",
      "Generating ProjectedGradientDescent attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93a19aa912a54150a0a8f3b5fb1d6ed4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PGD - Iterations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e662a98cfc7346ad8cabf8ed2b5d30b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:04<00:01,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for ProjectedGradientDescent attack saved for Ericsson-IDS model\n",
      "Generating BasicIterativeMethod_0.2 attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PGD - Random Initializations:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20a81e7076a64b9fad639ff6a115f479"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PGD - Iterations:   0%|          | 0/100 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fda5e1ba3bfe4601bc7523da45566916"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:04<00:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for BasicIterativeMethod_0.2 attack saved for Ericsson-IDS model\n",
      "Generating FastGradientMethod_0.2 attack dataset\n",
      "[SUCCESS] Adversarial data for FastGradientMethod_0.2 attack saved for Ericsson-IDS model\n",
      "Generating CarliniL2Method_0.2 attack dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C&W L_2:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fed9fc2f017248c68b2d48a291afa744"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:06<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Adversarial data for CarliniL2Method_0.2 attack saved for Ericsson-IDS model\n",
      "[SUCCESS] All the data is packed and saved successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.256655100Z",
     "start_time": "2024-07-10T22:42:40.249295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Document all the attack dataset types by reading the art documentation under evasion attacks\n",
    "# LowProFool is a specific attack for tabular data ... Try to implement it for the DLCB dataset\n",
    "# ThresholdAttack - This attack requires estimator clip values to be defined. try to solve this\n",
    "# Adversarial Robustness Assessment: Why both L0 and L∞ Attacks Are Necessary (https://arxiv.org/abs/1906.06026)\n",
    "# ShadowAttack - Breaking certified defenses: Semantic adversarial examples with spoofed robustness certificates\n",
    "# TargetedUniversalPerturbation - Calculates a universal adversarial perturbation (UAP) but uses fgsm at the backend (can change) https://arxiv.org/abs/1911.06502\n",
    "# UniversalPerturbation - Calculates a universal adversarial perturbation (UAP) but uses fgsm at the backend (can change) https://arxiv.org/abs/1610.08401\n",
    "# Wasserstein seem to run into an overflow error in _conjugate_sinkhorn for exponential beta when generating data.\n",
    "# JSMA SaliencyMapMethod took a long time to run even one datapoint. Stopped after one data point.\n",
    "# UniversalPerturbation shows like it creates only a small dataset in the logs. TargetedUniversalPerturbation weren't even shown on logs"
   ],
   "id": "f26d3cd90f48bfd2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attack data loading",
   "id": "5235c466c8b68e16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.257657600Z",
     "start_time": "2024-07-10T22:42:40.281467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create separate arrays for attack data testing\n",
    "\n",
    "# loading all the datasets\n",
    "\n",
    "# show all the files in results/*/AdvData\n",
    "\n",
    "def load_all_adv_data(data_dir):\n",
    "    return {f.split('/')[-1].split('_')[5]: np.load(f) for f in glob.glob(os.path.join(data_dir, 'adv_*.npy'))}\n",
    "\n",
    "adv_data = load_all_adv_data(load_all_adv_data_dir)\n",
    "unseen_attack = {'CarliniL2Method': adv_data.pop('CarliniL2Method')}"
   ],
   "id": "dff599454806134c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.257657600Z",
     "start_time": "2024-07-10T22:42:40.366029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#concatenate the adversarial data\n",
    "def concatenate_adv_data(adv_data_dict):\n",
    "    adv_data_ls = []\n",
    "    labels_ls = []\n",
    "    label_map = {}\n",
    "    for i, (key, value) in enumerate(adv_data_dict.items()):\n",
    "        adv_data_ls.append(value)\n",
    "        labels_ls.append(np.ones((value.shape[0],1), dtype=int)*int(i+1))\n",
    "        label_map[i+1] = key\n",
    "    X_full = np.concatenate(adv_data_ls, axis=0)\n",
    "    y_full = np.concatenate(labels_ls, axis=0) \n",
    "    return X_full, y_full, label_map\n",
    "\n",
    "full_adv_X, full_adv_y, label_map = concatenate_adv_data(adv_data)\n",
    "np.unique(full_adv_y, return_counts=True)"
   ],
   "id": "2a2f1f2641dd9023",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5, 6, 7]),\n",
       " array([10897, 10897, 10897, 10897, 10897, 10897, 10897]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.278448700Z",
     "start_time": "2024-07-10T22:42:40.416337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dmaddata_xai = DmadDatasets(data_directories)\n",
    "adv_det_datasets = dmaddata_xai.adv_detector_task(test_size=0.2, validation_size=None, verbose=True,\n",
    "                                                  multidim_labels=False)"
   ],
   "id": "dcd2ca35395ba72f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3484888/3799587416.py:249: DeprecatedWarning: _create_detector_labels is deprecated as of 0.1.0. Use the create_detector_full_ds_adding_labels for multiple adversarial and benign dataset\n",
      "  det_In, det_out = DmadDatasets._create_detector_labels(self.adv_X, self.dlcb_X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (87169, 256)\n",
      "X_test: (21793, 256)\n",
      "y_train: (87169,)\n",
      "y_test: (21793,)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.284687600Z",
     "start_time": "2024-07-10T22:42:42.001932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "benign_x = adv_det_datasets['X_train'][:full_adv_X.shape[0]]\n",
    "benign_y = np.zeros((benign_x.shape[0],1), dtype=int)\n",
    "label_map[0] = 'Benign'\n",
    "X_full = np.concatenate([benign_x, full_adv_X], axis=0)\n",
    "y_full = np.concatenate([benign_y, full_adv_y], axis=0)"
   ],
   "id": "67f5abb98d8a9e07",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.285681500Z",
     "start_time": "2024-07-10T22:42:42.108565Z"
    }
   },
   "cell_type": "code",
   "source": "np.unique(y_full, return_counts=True)",
   "id": "820add948d244c7f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " array([76279, 10897, 10897, 10897, 10897, 10897, 10897, 10897]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T00:44:11.308830400Z",
     "start_time": "2024-07-10T22:42:42.118648Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)\n",
   "id": "534687a355154dd5",
   "outputs": [],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
